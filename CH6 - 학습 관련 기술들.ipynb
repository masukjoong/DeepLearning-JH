{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31da038f",
   "metadata": {},
   "source": [
    "# 6. 학습 관련 기술들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd82167c",
   "metadata": {},
   "source": [
    "<!--  수식 -->\n",
    "<!-- <img src=\"deep_learning_images/e 1.1.png\">  -->\n",
    "<!-- 그림 -->\n",
    "<!-- <img src=\"deep_learning_images/fig 6-3.png\" width=\"500\" height=\"400\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a80f1",
   "metadata": {},
   "source": [
    "## 매개변수 갱신\n",
    "\n",
    "- 최적화(optimization)\n",
    "  \n",
    "  - 매개변수의 최적값을 찾는 것\n",
    "  \n",
    "  - 심층 신경망에서는 매개변수의 수가 매우 많아져서 비용이 높음\n",
    "  \n",
    "- 확률적 경사 하강법(SGD)\n",
    "  \n",
    "  - 매개변수의 기울기(미분)를 사용\n",
    "  \n",
    "  - 기울어진 방향으로 값을 반복 갱신해서 최적값을 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b176689a",
   "metadata": {},
   "source": [
    "## 확률적 경사 하강법(SGD)\n",
    "  \n",
    "### 수식\n",
    "\n",
    "<img src=\"deep_learning_images/e 6.1.png\"> \n",
    "  \n",
    "### 단점\n",
    "\n",
    "\n",
    "<img src=\"deep_learning_images/e 6.2.png\"> \n",
    "\n",
    "- 위와 같은 수식을 그림으로 나타내면\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-1.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "- 위처럼 나타나고 최솟값이 되는 장소는 (x, y)가 (0, 0)이지만\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-2.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "- 기울기의 대부분은 (0,0) 방향을 가리키지 않음\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-3.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "- 최솟값인 (0, 0)까지 지그재그로 비효율적으로 움직이는 것을 보여줌\n",
    "\n",
    "- 즉 SGD의 단점은 비등방성(anisotropy) 함수 (방향에 따라 성질, 위 그림에서는 기울기가 달라지는 함수) 에서 탐색경로가 비효율적임\n",
    "\n",
    "- 이러한 단점을 개선해주는 방식으로 Momentum, AdaGrad, Adam 방법이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27cc87",
   "metadata": {},
   "source": [
    "## 모멘텀(Momentum)\n",
    "\n",
    "<img src=\"deep_learning_images/e 6.3.png\"> \n",
    "\n",
    "<img src=\"deep_learning_images/e 6.4.png\"> \n",
    "\n",
    "- W는 갱신할 가중치 매개변수\n",
    "\n",
    "- ∂L/∂W는 W에 대한 손실함수의 기울기\n",
    "\n",
    "- n은 학습률\n",
    "\n",
    "- v는 물리에서 말하는 속도(velocity)에 해당\n",
    "\n",
    "- av는 물체가 아무런 힘을 받지 않을 때 서서히 하강시키는 역할을 함\n",
    "\n",
    "  물리에서의 지면 마찰이나 공기저항에 해당\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-4.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-5.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "- 모멘텀의 갱신경로는 공이 그릇 바닥을 구르듯 움직임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37758aa",
   "metadata": {},
   "source": [
    "## AdaGrad\n",
    "\n",
    "- 신경망 학습에서 학습률값이 중요함\n",
    "\n",
    "  - 너무 작으면 학습 시간이 길어짐\n",
    "  \n",
    "  - 너무 크면 학습이 제대로 이루어 지지 않음\n",
    "  \n",
    "- 학습률을 정하는 효과적 기술로 학습률 감소(learning rate decay)가 있음\n",
    "\n",
    "- 학습을 진행하면서 학습률을 점차 줄여가는 방법\n",
    "\n",
    "- 학습률을 서서히 낮추는 가장 간단한 방법은 매개변수 '전체'의 학습률 값을 일괄적으로 낮추는 것임\n",
    "\n",
    "- 학습률 감소방법을 발전 시킨게 AdaGrad\n",
    "\n",
    "- '각각의' 매개변수에 '맞춤형' 값을 만들어 줌\n",
    "\n",
    "- 개별 매개변수에 적응적으로(adaptive) 학습률을 조정\n",
    "\n",
    "- 하지만 학습이 진행될수록 갱신강도가 약해짐\n",
    "\n",
    "  이를 해결하기 위해 과거의 기울기는 서서히 잊고 기울기 정보를 반영하는 RMSProp이라는 방법이 있음\n",
    "  \n",
    "  지수이동평균(Exponential Moving Average, EMA)라 할여, 과거 기울기의 반영 규모를 기하급수적으로 감소시킴\n",
    "  \n",
    "<img src=\"deep_learning_images/e 6.5.png\"> \n",
    "\n",
    "<img src=\"deep_learning_images/e 6.6.png\"> \n",
    "\n",
    "- W는 갱신할 가중치 매개변수\n",
    "\n",
    "- ∂L/∂W은 W에 대한 손실함수의 기울기\n",
    "\n",
    "- n은 학습률\n",
    "\n",
    "- h는 기존 기울기 값을 제곱하여 더해주고 매개변수 갱신시 1/√h을 공해 학습률을 조정\n",
    "\n",
    "- 매개변수의 원소 중에서 많이 움직인(크게 갱신된) 원소는 학습률이 낮아짐\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-6.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac659c2",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "- Momentom과 AdaGrad를 융합한 듯한 방법\n",
    "\n",
    "- 하이퍼 파라미터의 '편향 보정'이 진행\n",
    "\n",
    "- 하이퍼파라미터는 3개를 설정함\n",
    "\n",
    "  - 학습률\n",
    "  \n",
    "  - 1차 Momentom 계수\n",
    "  \n",
    "  - 2차 Momentom 계수\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-7.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ea2ad",
   "metadata": {},
   "source": [
    "## 최적화 기법 비교\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-8.png\" width=\"800\" height=\"800\">\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-9.png\" width=\"800\" height=\"800\">\n",
    "\n",
    "- 위 기법들중 특출난 기법은 없음\n",
    "\n",
    "- 각자의 장단이 있어 잘 푸는 문제와 서툰 문제가 존재함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e1c75",
   "metadata": {},
   "source": [
    "## 가중치의 초깃값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dfe499",
   "metadata": {},
   "source": [
    "### 초깃값을 0으로 했을 때\n",
    "\n",
    "- 가중치가 0이기 때문에 두번째 층의 뉴런에 모두 같은 값이 전달됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f05f0",
   "metadata": {},
   "source": [
    "\n",
    "### 가중치를 표준편차가 1인 정규분포로 초기화 할 때\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-10.png\">\n",
    "\n",
    "- 위 그림에서 사용한 시그모이드 활성화 함수의 출력이 0 또는 1에 가까워 지면\n",
    "\n",
    "  역전파의 기울기 값이 점점 작아지다가 사라짐\n",
    "  \n",
    "- 이것을 기울기 소실(gradient vanishing)이라고 함\n",
    "\n",
    "\n",
    "### 가중치를 표준편차가 0.01인 정규분포로 초기화 할 때\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-11.png\">\n",
    "\n",
    "- 0.5에 치우쳐지는 것 처럼 뉴런들이 거의 값은 값을 출력함\n",
    "\n",
    "- 이런 경우 뉴런 1개짜리와 다름이 없음\n",
    "\n",
    "- 활성화 값들이 치우치면 표현력을 제한한다는 관점에서 문제가 됨\n",
    "\n",
    "- 각 층의 활성화 값은 적당히 고루 분포되어야 학습이 효율적으로 이루어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c39ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5ae67cc",
   "metadata": {},
   "source": [
    "### Xavier 초기값\n",
    "\n",
    "- 일반적인 딥러닝 프레임워크들이 표준적으로 이용하고 있음\n",
    "\n",
    "- 각 층의 활성화값들을 광범위하게 분포하려고 했는데\n",
    "\n",
    "  앞 계층의 노드가 n개라면 표준편차가 1/√n인 분포를 사용하면 된다는 결론을 냄\n",
    "  \n",
    "- 활성화 함수가 선형인 것을 전제로 함\n",
    "  \n",
    "<img src=\"deep_learning_images/fig 6-12.png\"  width=\"500\" height=\"500\">\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-13.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa292508",
   "metadata": {},
   "source": [
    "### He 초기값\n",
    "\n",
    "- ReLU를 이용할때 사용함\n",
    "\n",
    "- 앞 계층의 노드가 n개라면 표준편차가 √2/n인 분포를 사용\n",
    "\n",
    "- ReLU는 음의 영역이 0이라서 더 넓게 분포시키기 위해 2배의 계수로 만들기 위해 사용\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-14.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e50805",
   "metadata": {},
   "source": [
    "### MNIST 데이터 셋으로 본 가중치 초깃값 비교\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-15.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "- 층별 뉴런 개수가 100개인 5층 신경망\n",
    "\n",
    "- 활성화 함수로 ReLU를 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fed156",
   "metadata": {},
   "source": [
    "## 배치 정규화(Batch Normalization)\n",
    "\n",
    "- 각 층의 활성화값 분포가 적당히 퍼지면 학습이 원활이 수행됨\n",
    "- 각 층이 활성화를 적당히 퍼뜨리도록 '강제'하는 아이디어로 시작된 방법\n",
    "- 2015년에 제안된 얼마 안된 방법이지만 아래와 같은 이유로 주목받음\n",
    "  - 학습을 빨리 진행할 수 있음(학습속도개선)\n",
    "  - 초깃값에 크게 의존하지 않음\n",
    "  - 오버피팅을 억제함(드롭아웃 등의 필요성 감소)\n",
    "  \n",
    "- 배치 정규화를 사용한 신경망의 예\n",
    "<img src=\"deep_learning_images/fig 6-16.png\">\n",
    "\n",
    "- 배치 정규화는 학습 시 미니배치를 단위로 정규화 함\n",
    "\n",
    "- 데이터 분포가 평균이 0, 분산이 1이 되도록 정규화함\n",
    "\n",
    "  수식은 아래와 같음\n",
    "  \n",
    "  <img src=\"deep_learning_images/e 6.7.png\"> \n",
    "\n",
    "- 미니배치 B라는 m개의 입력데이터의 집합데 애해 평균 μB와 분산 σ²B를 구함\n",
    "\n",
    "- 입력 데이터를 평균이 0, 분산이 1이 되게(적절한 분포가 되게) 정규화함\n",
    "\n",
    "- ε(epsilon)은 작은값(10e-7등)으로, 0으로 나누는 사태 예방\n",
    "\n",
    "- 미니배치 입력 데이터를 평균 0, 분산 1인 데이터로 변환하는 일을 함\n",
    "\n",
    "- 이 처리를활성화 함수의 앞(또는 뒤)에 삽입함으로써 데이터 분포가 덜 치우치게 할 수 있음\n",
    "\n",
    "- 배치 정규화 층마다 이 정규화된 데이터에 고유한 확대(scale)와 이동(shift)변환을 수행\n",
    "\n",
    "  <img src=\"deep_learning_images/e 6.8.png\"> \n",
    "  \n",
    "- γ가 확대를 β가 이동을 담당함\n",
    "\n",
    "- 두 값은 각각 1, 0 부터 시작하고 학습하면서 적합한 값으로 조정해감\n",
    "\n",
    "- 이 알고리즘이 신경망에서 순전파 때 적용되는데 5장에서 설명한 계산그래프로는 아래와 같음\n",
    "\n",
    "- 더 알고싶으면 프레드릭 크레저트(Frederic Kratzert)의 블로그 참조\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-17.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c38a6",
   "metadata": {},
   "source": [
    "### 배치 정규화의 효과\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-18.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "<img src=\"deep_learning_images/fig 6-19.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43c63e",
   "metadata": {},
   "source": [
    "## 오버피팅\n",
    "\n",
    "- 훈련 데이터에만 지나치게 적응되어 그 외의 데이터에는 제대로 대응하지 못하는 상태를 말함\n",
    "\n",
    "- 주로 두가지 경우에서 발생\n",
    "\n",
    "  - 매개변수가 많고 표현력이 높은 모델\n",
    "  \n",
    "  - 훈련 데이터가 적을때"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f28024",
   "metadata": {},
   "source": [
    "### 가중치 감소(weight decay)\n",
    "\n",
    "- 오버피팅 업제용으로 많이 이용해온 방법\n",
    "\n",
    "- 학습 과정에서 큰 가중치에 대해서는 그에 상응하는 큰 페널티를 부과하여\n",
    "\n",
    "  오버피팅을 억제하는 방법\n",
    "  \n",
    "- 가중치 감소는 각각의 손실함수에 1/2λW²을 더함\n",
    "\n",
    "- λ는 정규화의 세기를 조절하는 하이퍼파라미터\n",
    "\n",
    "  크게 설정할 수록 큰 가중치에 대한 패널치가 커짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1cbd1e",
   "metadata": {},
   "source": [
    "### 드롭아웃(Dropout)\n",
    "\n",
    "- 뉴런을 임의로 삭제하면서 학습하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd4f1b",
   "metadata": {},
   "source": [
    "### 앙상블 학습(ensemble learning)\n",
    "\n",
    "- 개별적으로 학습시킨 여러 모델의 출력을 평균 내어 추론하는 방식\n",
    "\n",
    "- 예를들어 같거나 비슷한 구조의 네트워크를 5개 준비하여 따로따로 학습시키고 평균을 내어 답하는 것\n",
    "\n",
    "- 이런 앙상블 학습은 드롭아웃과 밀접함\n",
    "\n",
    "- 드롭아웃이 학습 때 뉴런을 무작위로 삭제하는 행위를 매번 다른 모델을 학습시키는 것으로 해석할 수 있기 때문\n",
    "\n",
    "- 추론 때는 뉴런의 출력에 삭제한 비율을 곱함으로써 평균을 내는 것과 같은 효과를 얻음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e19a4",
   "metadata": {},
   "source": [
    "## 적합한 하이퍼파라미터 탐색\n",
    "\n",
    "### 검증 데이터\n",
    "\n",
    "- 훈련데이터, 시험데이터로 분류해서 학습 할때 하이퍼파라미터의 성능평가는 시험데이터를 사용하면 안됨\n",
    "\n",
    "- 시험데이터를 사용하여 하이퍼파라미터를 조정하면 시험 데이터에 오버피팅되기 떄문\n",
    "\n",
    "- 하이퍼파라미터 조정용 데이터가 필요한데 이를 검증 데이터(validation data)라고 부름\n",
    "\n",
    "- 데이터 종류\n",
    "\n",
    "  - 훈련 데이터: 매개변수 학습\n",
    "  \n",
    "  - 검증 데이터: 하이퍼파라미터 성능 평가\n",
    "  \n",
    "  - 시험 데이터: 신경망의 범용 성능 평가\n",
    "  \n",
    "### 하이퍼파라미터 최적화\n",
    "\n",
    "- '최적값'이 존재하는 범위를 조금씩 줄여가는 것이 핵심\n",
    "\n",
    "- 그리드 서치(grid search)같은 규칙적인 탐색보다는 무작위로 샘플링해 탐색하는 편이 좋은 결과를 낸다함\n",
    "\n",
    "- 하이퍼파라미터는 대략적으로 지정하는 것이 효과적\n",
    "\n",
    "- 0.001에서 1,000 사이(10의3제곱~10의3제곱)와 같이 '10의 거듭제곱'단위로 범위를 지정함\n",
    "\n",
    "- 이를 '로그스케일(log scale)로 지정한다고 함\n",
    "\n",
    "- 매우 오랜 시간이 걸리기 때문에 학습을 위한 epoch를 작게 하여 1회 평가에 걸리는 시간을 단축하는게 효과적임\n",
    "\n",
    "- 순서\n",
    "\n",
    "  0. 하이퍼파라미터 값의 범위를 설정함\n",
    "  1. 설정된 범위에서 하이퍼파라미터의 값을 무작위로 추출\n",
    "  2. 1단계에서 샘플링한 하이퍼파라미터 값을 사용하여 학습하고, 검증데이터로 정확도를 평가함\n",
    "     (단, epoch는 작게 설정)\n",
    "  3. 1단계와 2단계를 특정 횟수(100회 등) 반복하며, 그 정확도의 결과를 보고 하이퍼파라미터의 범위를 좁힘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b225d366",
   "metadata": {},
   "source": [
    "## optimizer python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63c1e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key] \n",
    "            \n",
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]\n",
    "            \n",
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7f6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
